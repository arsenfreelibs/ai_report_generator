# -*- coding: utf-8 -*-
"""Training (SL2 API QLoRA)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjkiil9OidT5p4PLeNwtOVebq6ct32Qs

# SL2 Query API Model Fine-tuning with QLoRA
# Google Colab Notebook 2: Model Training

'''
This notebook fine-tunes deepseek-coder-6.7b-instruct model on SL2 Query API examples
using QLoRA (Quantized LoRA) for memory-efficient training.

Prerequisites:
1. Run the dataset creation notebook first
2. Have datasets ready in Google Drive or Hugging Face Hub
3. Use GPU runtime (T4 or better) in Google Colab
'''

## 1. SETUP AND IMPORTS
"""

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("‚ö†Ô∏è No GPU detected. Training will be very slow on CPU.")

# Install required packages
!pip install -q torch torchvision torchaudio
!pip install -q transformers>=4.36.0
!pip install -q datasets>=2.15.0
!pip install -q peft>=0.7.0
!pip install -q trl>=0.7.0
!pip install -q bitsandbytes>=0.41.0
!pip install -q accelerate>=0.24.0
!pip install -q huggingface_hub
!pip install -q wandb  # For experiment tracking (optional)

# Import libraries
import os
import json
import torch
from datetime import datetime
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from trl import SFTTrainer
from huggingface_hub import notebook_login
import pandas as pd
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

# Login to Hugging Face
notebook_login()

# Optional: Login to Weights & Biases for experiment tracking
# import wandb
# wandb.login()

"""## 2. CONFIGURATION"""

# Model configuration
MODEL_NAME = "deepseek-ai/deepseek-coder-6.7b-instruct"
NEW_MODEL_NAME = "deepseek-coder-6.7b-sl2-query-api"

# Dataset configuration
DATASET_PATH = "/content/gdrive/MyDrive/Colab Notebooks/datasets/sl2-query-api-dataset-alpaca"
# Alternative: Load from Hugging Face Hub
# DATASET_NAME = "your-username/sl2-query-api-dataset-alpaca"

# Training configuration - Models stored on Google Drive
OUTPUT_DIR = "/content/gdrive/MyDrive/Colab Notebooks/models"
LOGS_DIR = "/content/gdrive/MyDrive/Colab Notebooks/logs"

# Alternative storage options
GDRIVE_MODEL_DIR = "/content/gdrive/MyDrive/AI_Models/SL2_QueryAPI"
GDRIVE_BACKUP_DIR = "/content/gdrive/MyDrive/AI_Models/Backups"

# Create all directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(GDRIVE_MODEL_DIR, exist_ok=True)
os.makedirs(GDRIVE_BACKUP_DIR, exist_ok=True)

# QLoRA configuration
LORA_R = 64
LORA_ALPHA = 16
LORA_DROPOUT = 0.1
LORA_TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]

# Training hyperparameters
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 3
MAX_SEQ_LENGTH = 2048
WARMUP_RATIO = 0.03
WEIGHT_DECAY = 0.001
LOGGING_STEPS = 10
SAVE_STEPS = 100
EVAL_STEPS = 100

# Create directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

print("Configuration:")
print(f"  Model: {MODEL_NAME}")
print(f"  New model name: {NEW_MODEL_NAME}")
print(f"  Output directory: {OUTPUT_DIR}")
print(f"  Max sequence length: {MAX_SEQ_LENGTH}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Learning rate: {LEARNING_RATE}")
print(f"  Epochs: {NUM_EPOCHS}")

"""## 3. LOAD DATASET"""

def load_training_dataset():
    """Load the training dataset."""

    try:
        # Try loading from local drive first
        if os.path.exists(DATASET_PATH):
            print(f"Loading dataset from local path: {DATASET_PATH}")
            dataset = load_from_disk(DATASET_PATH)
        else:
            # Load from Hugging Face Hub
            print(f"Loading dataset from Hugging Face Hub: {DATASET_NAME}")
            dataset = load_dataset(DATASET_NAME)

        print(f"‚úÖ Dataset loaded successfully!")
        print(f"  Train samples: {len(dataset['train'])}")
        print(f"  Validation samples: {len(dataset['validation'])}")
        print(f"  Test samples: {len(dataset['test'])}")

        return dataset

    except Exception as e:
        print(f"‚ùå Error loading dataset: {e}")
        return None

dataset = load_training_dataset()
if dataset is None:
    print("Cannot proceed without dataset. Please check the dataset path.")
    exit()

# Preview dataset
print("\nüîç Dataset sample:")
sample = dataset['train'][0]
for key, value in sample.items():
    print(f"  {key}: {value}")

"""## 4. SETUP QUANTIZATION CONFIG"""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("‚úÖ Quantization config created")

"""## 5. LOAD MODEL AND TOKENIZER"""

print(f"Loading model: {MODEL_NAME}")
print("This may take a few minutes...")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    padding_side="right"
)

# Add pad token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# Disable cache for training
model.config.use_cache = False
model.config.pretraining_tp = 1

print("‚úÖ Model and tokenizer loaded successfully")
print(f"  Model device: {next(model.parameters()).device}")
print(f"  Model dtype: {next(model.parameters()).dtype}")

"""## 6. SETUP LORA CONFIG"""

# LoRA configuration
peft_config = LoraConfig(
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    r=LORA_R,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=LORA_TARGET_MODULES,
)

print("‚úÖ LoRA config created")
print(f"  LoRA rank (r): {LORA_R}")
print(f"  LoRA alpha: {LORA_ALPHA}")
print(f"  LoRA dropout: {LORA_DROPOUT}")
print(f"  Target modules: {LORA_TARGET_MODULES}")

"""## 7. PREPARE TRAINING DATA"""

def format_instruction(sample):
    """Format the sample for instruction following."""

    instruction = sample['instruction']
    input_text = sample['input']
    output = sample['output']

    # Create prompt template
    if input_text:
        prompt = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
    else:
        prompt = f"""### Instruction:
{instruction}

### Response:
{output}"""

    return {"text": prompt}

# Format datasets
train_dataset = dataset['train'].map(format_instruction)
eval_dataset = dataset['validation'].map(format_instruction)

print("‚úÖ Training data formatted")
print(f"  Training samples: {len(train_dataset)}")
print(f"  Evaluation samples: {len(eval_dataset)}")

# Preview formatted sample
print("\nüîç Formatted sample:")
print(train_dataset[0]['text'][:500] + "...")

"""## 8. SETUP TRAINING ARGUMENTS"""

# Generate unique run name
run_name = f"sl2-query-api-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
output_dir = os.path.join(OUTPUT_DIR, run_name)

training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=LOGGING_STEPS,
    learning_rate=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY,
    fp16=False,
    bf16=True,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=WARMUP_RATIO,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",  # or "wandb" if using wandb
    eval_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_strategy="steps",
    save_steps=SAVE_STEPS,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    push_to_hub=False,  # Set to True if you want to push to hub during training
    hub_model_id=f"arsen-r-a/{NEW_MODEL_NAME}",  # Update with your username
    logging_dir=os.path.join(LOGS_DIR, run_name),
)

print("‚úÖ Training arguments configured")
print(f"  Output directory: {output_dir}")
print(f"  Run name: {run_name}")

"""## 9. SETUP TRAINER"""

# Check TRL version and available parameters
try:
    import trl
    print(f"TRL version: {trl.__version__}")
except:
    print("Could not determine TRL version")

# Format dataset for training (simple text format)
def format_dataset_simple(dataset):
    """Simple dataset formatting."""
    def format_example(example):
        return {"text": example["text"]}
    return dataset.map(format_example)

# Format datasets
print("Formatting datasets...")
simple_train_dataset = format_dataset_simple(train_dataset)
simple_eval_dataset = format_dataset_simple(eval_dataset)

print("Applying PEFT adapters to model...")
model = get_peft_model(model, peft_config)
print("‚úÖ PEFT adapters applied to model")

# Create trainer with absolute minimal parameters
print("Creating trainer with minimal parameters...")
try:
    # Try with basic parameters first
    trainer = SFTTrainer(
        model=model,
        train_dataset=simple_train_dataset,
        eval_dataset=simple_eval_dataset,
        args=training_arguments,
    )
    print("‚úÖ Trainer initialized successfully")

except Exception as e:
    print(f"‚ùå Error with SFTTrainer: {e}")
    print("Trying fallback with standard Trainer...")

    # Fallback to standard Trainer if SFTTrainer fails
    from transformers import Trainer

    def tokenize_function(examples):
        """Tokenize the texts."""
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=MAX_SEQ_LENGTH,
        )

    # Tokenize datasets
    tokenized_train = simple_train_dataset.map(tokenize_function, batched=True)
    tokenized_eval = simple_eval_dataset.map(tokenize_function, batched=True)

    # Create standard trainer
    trainer = Trainer(
        model=model,
        args=training_arguments,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        tokenizer=tokenizer,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        ),
    )
    print("‚úÖ Fallback Trainer initialized successfully")

"""## 10. START TRAINING"""

print("\n" + "="*50)
print("STARTING TRAINING")
print("="*50)

print(f"Training will start now. Expected duration: ~{NUM_EPOCHS * len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) // 60} minutes")
print("You can monitor progress in TensorBoard or the logs.")

# Start training
try:
    trainer.train()
    print("‚úÖ Training completed successfully!")
except Exception as e:
    print(f"‚ùå Training failed: {e}")
    raise

"""## 11. SAVE MODEL TO GOOGLE DRIVE"""

print("\nSaving trained model to Google Drive...")

# Save the final model to multiple locations for backup
final_model_path = os.path.join(output_dir, "final_model")
gdrive_model_path = os.path.join(GDRIVE_MODEL_DIR, f"{NEW_MODEL_NAME}-{run_name}")
backup_path = os.path.join(GDRIVE_BACKUP_DIR, f"{NEW_MODEL_NAME}-{run_name}")

# Save to primary location
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)

# Copy to dedicated Google Drive model directory
import shutil
try:
    shutil.copytree(final_model_path, gdrive_model_path)
    print(f"‚úÖ Model copied to Google Drive: {gdrive_model_path}")
except Exception as e:
    print(f"‚ö†Ô∏è Failed to copy to Google Drive model dir: {e}")

# Create backup copy
try:
    shutil.copytree(final_model_path, backup_path)
    print(f"‚úÖ Backup created: {backup_path}")
except Exception as e:
    print(f"‚ö†Ô∏è Failed to create backup: {e}")

print(f"‚úÖ Model saved to Google Drive locations:")
print(f"  Primary: {final_model_path}")
print(f"  Models folder: {gdrive_model_path}")
print(f"  Backup: {backup_path}")

# Save model info for easy loading later
model_info = {
    "model_name": NEW_MODEL_NAME,
    "base_model": MODEL_NAME,
    "training_date": datetime.now().isoformat(),
    "run_name": run_name,
    "paths": {
        "primary": final_model_path,
        "gdrive": gdrive_model_path,
        "backup": backup_path
    },
    "config": {
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "learning_rate": LEARNING_RATE,
        "epochs": NUM_EPOCHS,
        "batch_size": BATCH_SIZE
    },
    "dataset_info": {
        "train_samples": len(dataset['train']),
        "val_samples": len(dataset['validation']),
        "test_samples": len(dataset['test'])
    }
}

# Save model info as JSON
info_path = os.path.join(GDRIVE_MODEL_DIR, f"{NEW_MODEL_NAME}-info.json")
with open(info_path, 'w') as f:
    json.dump(model_info, f, indent=2)

print(f"‚úÖ Model info saved: {info_path}")

"""## 12. GOOGLE DRIVE MODEL MANAGEMENT FUNCTIONS"""

def list_saved_models():
    """List all models saved in Google Drive."""

    print("\nüìÅ Models in Google Drive:")
    if not os.path.exists(GDRIVE_MODEL_DIR):
        print("  No models found.")
        return []

    models = []
    for item in os.listdir(GDRIVE_MODEL_DIR):
        item_path = os.path.join(GDRIVE_MODEL_DIR, item)
        if os.path.isdir(item_path):
            # Check if it's a model directory (contains config.json or adapter_config.json)
            if (os.path.exists(os.path.join(item_path, "adapter_config.json")) or
                os.path.exists(os.path.join(item_path, "config.json"))):
                models.append(item)

                # Get size
                size = sum(os.path.getsize(os.path.join(item_path, f))
                          for f in os.listdir(item_path)
                          if os.path.isfile(os.path.join(item_path, f)))
                size_mb = size / (1024 * 1024)

                print(f"  üì¶ {item} ({size_mb:.1f} MB)")

    return models

def load_model_from_gdrive(model_name: str):
    """Load a saved model from Google Drive."""

    model_path = os.path.join(GDRIVE_MODEL_DIR, model_name)

    if not os.path.exists(model_path):
        print(f"‚ùå Model not found: {model_path}")
        return None, None

    try:
        # Check if it's a LoRA model or merged model
        if os.path.exists(os.path.join(model_path, "adapter_config.json")):
            print("Loading LoRA model...")
            # Load base model first
            base_model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            # Apply LoRA
            model = PeftModel.from_pretrained(base_model, model_path)
            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        else:
            print("Loading merged model...")
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            tokenizer = AutoTokenizer.from_pretrained(model_path)

        print(f"‚úÖ Model loaded from Google Drive: {model_name}")
        return model, tokenizer

    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return None, None

def create_model_archive():
    """Create a compressed archive of the model for easy sharing."""

    import zipfile

    archive_name = f"{NEW_MODEL_NAME}-{run_name}.zip"
    archive_path = os.path.join(GDRIVE_BACKUP_DIR, archive_name)

    try:
        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            model_dir = gdrive_model_path if os.path.exists(gdrive_model_path) else final_model_path

            for root, dirs, files in os.walk(model_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, model_dir)
                    zipf.write(file_path, arcname)

        archive_size = os.path.getsize(archive_path) / (1024 * 1024)
        print(f"‚úÖ Model archive created: {archive_path} ({archive_size:.1f} MB)")
        return archive_path

    except Exception as e:
        print(f"‚ùå Failed to create archive: {e}")
        return None

# List existing models
list_saved_models()

# Ask if user wants to create archive
archive_choice = input("\nDo you want to create a compressed archive of the model? (y/n): ").lower().strip()
if archive_choice == 'y':
    create_model_archive()

"""## 12. MERGE AND SAVE FULL MODEL (OPTIONAL)"""

def merge_and_save_model():
    """Merge LoRA weights with base model and save."""

    print("Merging LoRA weights with base model...")

    # Reload base model in full precision for merging
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # Load LoRA model
    model_with_lora = PeftModel.from_pretrained(base_model, final_model_path)

    # Merge weights
    merged_model = model_with_lora.merge_and_unload()

    # Save merged model
    merged_path = os.path.join(output_dir, "merged_model")
    merged_model.save_pretrained(merged_path)
    tokenizer.save_pretrained(merged_path)

    print(f"‚úÖ Merged model saved to: {merged_path}")
    return merged_path

# Ask user if they want to merge
merge_choice = input("\nDo you want to merge LoRA weights with base model? This requires more memory but creates a standalone model. (y/n): ").lower().strip()

final_model_path = "/content/gdrive/MyDrive/Colab Notebooks/models/sl2-query-api-20250805-121855/final_model"
run_name = "sl2-query-api-20250805-121855"
output_dir = os.path.join(OUTPUT_DIR, run_name)
merged_path = None
if merge_choice == 'y':
    try:
        merged_path = merge_and_save_model()
    except Exception as e:
        print(f"‚ö†Ô∏è Merging failed: {e}")
        print("The LoRA model is still available for inference.")

"""## 13. TEST THE MODEL"""

print("\n" + "="*50)
print("TESTING THE TRAINED MODEL")
print("="*50)

def test_model():
    """Test the trained model with sample prompts."""

    # Load the trained model for inference
    if merged_path and os.path.exists(merged_path):
        test_model_path = merged_path
        print("Using merged model for testing...")

        # Load merged model
        test_model = AutoModelForCausalLM.from_pretrained(
            test_model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        test_tokenizer = AutoTokenizer.from_pretrained(test_model_path)

    else:
        test_model_path = final_model_path
        print("Using LoRA model for testing...")

        # Load base model and apply LoRA
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        test_model = PeftModel.from_pretrained(base_model, test_model_path)
        test_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # Create pipeline
    pipe = pipeline(
        "text-generation",
        model=test_model,
        tokenizer=test_tokenizer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.1,
        top_p=0.95,
        pad_token_id=test_tokenizer.eos_token_id
    )

    # Test prompts
    test_prompts = [
        "### Instruction:\nWrite a SL2 Query API function for basic database queries with conditions\n\n### Response:",
        "### Instruction:\nCreate a JavaScript function using SL2 Query API to perform bulk updates on multiple records\n\n### Response:",
        "### Instruction:\nShow me how to group data and apply aggregation functions using SL2 Query API\n\n### Response:",
        "### Instruction:\nImplement a SL2 database query function that joins multiple tables\n\n### Response:"
    ]

    print("\nüß™ Testing trained model:")

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n--- Test {i} ---")
        print(f"Prompt: {prompt.split('### Instruction:')[1].split('### Response:')[0].strip()}")

        try:
            result = pipe(prompt)
            generated_text = result[0]['generated_text']
            response = generated_text.split("### Response:")[-1].strip()

            print(f"Generated Response:\n{response[:300]}...")

        except Exception as e:
            print(f"‚ùå Error generating response: {e}")

    return pipe

# Run tests
try:
    pipe = test_model()
    print("‚úÖ Model testing completed!")
except Exception as e:
    print(f"‚ö†Ô∏è Model testing failed: {e}")
    pipe = None

"""## 14. UPLOAD TO HUGGING FACE HUB"""

def upload_model_to_hub():
    """Upload the trained model to Hugging Face Hub."""

    model_to_upload = merged_path if merged_path else final_model_path
    model_type = "merged" if merged_path else "lora"

    hub_model_name = f"your-username/{NEW_MODEL_NAME}-{model_type}"  # Update with your username

    try:
        print(f"Uploading {model_type} model to Hugging Face Hub...")
        print(f"Model path: {model_to_upload}")
        print(f"Hub name: {hub_model_name}")

        # Load model and tokenizer
        if merged_path:
            upload_model = AutoModelForCausalLM.from_pretrained(
                model_to_upload,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
        else:
            # For LoRA model, we need to upload the adapter
            from peft import PeftModel
            base_model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
            upload_model = PeftModel.from_pretrained(base_model, model_to_upload)

        upload_tokenizer = AutoTokenizer.from_pretrained(model_to_upload)

        # Push to hub
        upload_model.push_to_hub(
            hub_model_name,
            private=False,  # Set to True for private model
            commit_description=f"SL2 Query API fine-tuned model ({model_type})"
        )

        upload_tokenizer.push_to_hub(
            hub_model_name,
            commit_description=f"SL2 Query API tokenizer ({model_type})"
        )

        print(f"‚úÖ Model uploaded successfully!")
        print(f"ü§ó Model URL: https://huggingface.co/{hub_model_name}")

        # Create model card
        create_model_card(hub_model_name, model_type)

    except Exception as e:
        print(f"‚ùå Upload failed: {e}")
        print("You can upload manually later using the saved model files.")

def create_model_card(model_name: str, model_type: str):
    """Create a model card for the uploaded model."""

    model_card = f"""---
language: javascript
license: apache-2.0
tags:
- code-generation
- javascript
- sl2
- query-api
- database
- fine-tuned
- {model_type}
base_model: {MODEL_NAME}
datasets:
- your-username/sl2-query-api-dataset
pipeline_tag: text-generation
---

# {NEW_MODEL_NAME.title()} ({model_type.upper()})

This model is a fine-tuned version of [{MODEL_NAME}](https://huggingface.co/{MODEL_NAME}) on SL2 Query API examples.

## Model Description

This model has been fine-tuned to generate JavaScript code specifically for SL2 Query API operations. It can help developers write efficient database queries, perform data operations, and implement complex business logic using the SL2 JavaScript API.

## Training Details

- **Base Model**: {MODEL_NAME}
- **Fine-tuning Method**: QLoRA (Quantized LoRA)
- **Training Epochs**: {NUM_EPOCHS}
- **Learning Rate**: {LEARNING_RATE}
- **Batch Size**: {BATCH_SIZE}
- **LoRA Rank**: {LORA_R}
- **LoRA Alpha**: {LORA_ALPHA}

## Capabilities

The model can generate code for:
- Basic database queries with conditions
- Array operators (RTL, Multichoice)
- Relationship queries (RTL, Ref, GlobalRef)
- Data ordering and sorting
- Complex table joins
- Data grouping and aggregation
- Bulk operations (update, delete, insert)
- Pagination and limits
- Iterator operations for large datasets

## Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained('{model_name}')
model = AutoModelForCausalLM.from_pretrained(
    '{model_name}',
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

prompt = '''### Instruction:
Write a SL2 Query API function for basic database queries with conditions

### Response:'''

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## Example Generations

### Basic Query
**Prompt**: Write a SL2 Query API function for basic database queries with conditions

**Generated**:
```javascript
const basicFind = async () => {{
    const model = await p.getModel('user');

    const users = await model.find({{ active: true }});
    const activeAdmins = await model.find({{
        active: true,
        role: 'admin'
    }});

    return {{ users, activeAdmins }};
}};
```

## Training Data

The model was trained on a custom dataset of SL2 Query API examples covering:
- {len(dataset['train'])} training examples
- {len(dataset['validation'])} validation examples
- {len(dataset['test'])} test examples

## Limitations

- Specialized for SL2 Query API only
- May not generalize well to other JavaScript frameworks
- Requires understanding of SL2 database concepts

## License

Apache 2.0
"""

    try:
        from huggingface_hub import HfApi
        api = HfApi()
        api.upload_file(
            path_or_fileobj=model_card.encode(),
            path_in_repo="README.md",
            repo_id=model_name,
            repo_type="model"
        )
        print("‚úÖ Model card created successfully!")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not create model card: {e}")

# Ask user if they want to upload
upload_choice = input("\nDo you want to upload the model to Hugging Face Hub? (y/n): ").lower().strip()

if upload_choice == 'y':
    upload_model_to_hub()
else:
    print("Skipping upload. You can upload manually later.")

"""## 15. INTERACTIVE TESTING"""

def interactive_testing():
    """Interactive testing interface."""

    if pipe is None:
        print("‚ö†Ô∏è Model pipeline not available for interactive testing.")
        return

    print("\n" + "="*50)
    print("INTERACTIVE TESTING")
    print("="*50)
    print("Enter your prompts to test the model. Type 'exit' to quit.")

    while True:
        user_prompt = input("\nüí≠ Enter your instruction: ").strip()

        if user_prompt.lower() in ['exit', 'quit', 'q']:
            break

        if not user_prompt:
            continue

        # Format prompt
        formatted_prompt = f"### Instruction:\n{user_prompt}\n\n### Response:"

        try:
            print("ü§ñ Generating response...")
            result = pipe(formatted_prompt)
            generated_text = result[0]['generated_text']
            response = generated_text.split("### Response:")[-1].strip()

            print(f"\n‚ú® Generated Response:\n{response}")

        except Exception as e:
            print(f"‚ùå Error: {e}")

    print("Interactive testing ended.")

# Ask user if they want interactive testing
interactive_choice = input("\nDo you want to try interactive testing? (y/n): ").lower().strip()

if interactive_choice == 'y':
    interactive_testing()

"""## 16. TRAINING SUMMARY"""

print("\n" + "="*60)
print("TRAINING COMPLETED SUCCESSFULLY! üéâ")
print("="*60)

print(f"\nüìä Training Summary:")
print(f"  Base Model: {MODEL_NAME}")
print(f"  Training Method: QLoRA")
print(f"  Epochs: {NUM_EPOCHS}")
print(f"  Training Samples: {len(dataset['train'])}")
print(f"  Validation Samples: {len(dataset['validation'])}")
print(f"  Learning Rate: {LEARNING_RATE}")
print(f"  Batch Size: {BATCH_SIZE}")

print(f"\nüìÅ Model Files:")
print(f"\nüìÅ Model Storage Locations:")
print(f"  Primary: {final_model_path}")
print(f"  Google Drive Models: {gdrive_model_path if 'gdrive_model_path' in locals() else 'Not created'}")
print(f"  Backup: {backup_path if 'backup_path' in locals() else 'Not created'}")
print(f"  Logs: {os.path.join(LOGS_DIR, run_name)}")

print(f"\nüîÑ Loading Models from Google Drive:")
print(f"```python")
print(f"# To load your model later:")
print(f"from transformers import AutoTokenizer, AutoModelForCausalLM")
print(f"from peft import PeftModel")
print(f"")
print(f"# For LoRA model:")
print(f"base_model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}')")
print(f"model = PeftModel.from_pretrained(base_model, '{gdrive_model_path if 'gdrive_model_path' in locals() else final_model_path}')")
print(f"tokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')")
print(f"```")

print(f"\nüöÄ Next Steps:")
print(f"  1. Test the model with your own SL2 Query API prompts")
print(f"  2. Deploy the model for code generation")
print(f"  3. Fine-tune further with domain-specific data")
print(f"  4. Share your model on Hugging Face Hub")

print(f"\nüí° Usage Example:")
print(f"```python")
print(f"from transformers import AutoTokenizer, AutoModelForCausalLM")
print(f"from peft import PeftModel")
print(f"")
print(f"# Load model")
print(f"base_model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}')")
print(f"model = PeftModel.from_pretrained(base_model, '{final_model_path}')")
print(f"tokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')")
print(f"")
print(f"# Generate code")
print(f"prompt = 'Write a SL2 Query API function for database queries'")
print(f"# ... generation code ...")
print(f"```")

print(f"\nüéØ Model is ready for SL2 Query API code generation!")
print("="*60)