# -*- coding: utf-8 -*-
"""Dataset Creation (SL2 API QLoRA)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIi97TVfEwnhzcItPlqPk2uJi7ugZpSw

# SL2 Query API Dataset Creation for Fine-tuning
# Google Colab Notebook 1: Dataset Creation

'''
This notebook creates a training dataset from SL2 Query API examples
for fine-tuning deepseek-coder-6.7b-instruct model using QLoRA.

Prerequisites:
1. Upload examples.md to /content/gdrive/MyDrive/Colab Notebooks/input/
2. Run this notebook to create the dataset
3. Upload dataset to Hugging Face Hub
'''

## Install required packages
"""

!pip install datasets transformers huggingface_hub pandas numpy

"""## Import Libraries"""

import os
import re
import json
import pandas as pd
from datasets import Dataset, DatasetDict
from huggingface_hub import notebook_login, HfApi
import numpy as np
from typing import List, Dict, Tuple
import random
from datetime import datetime

"""## Mount Google Drive and Access CSV File"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

"""## Login to Hugging Face"""

# Login to Hugging Face
notebook_login()

"""## File paths"""

EXAMPLES_FILE = "/content/gdrive/MyDrive/Colab Notebooks/input/examples.md"
OUTPUT_DIR = "/content/gdrive/MyDrive/Colab Notebooks/datasets"
DATASET_NAME = "sl2-query-api-dataset"

"""## Dataset configuration"""

DATASET_VERSION = "1.0"
HF_REPO_NAME = "arsen-r-a/sl2-query-api-dataset"  # Update with your username

# Create output directory
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"Examples file: {EXAMPLES_FILE}")
print(f"Output directory: {OUTPUT_DIR}")
print(f"Dataset will be saved as: {DATASET_NAME}")

"""## LOAD AND PARSE EXAMPLES"""

def load_examples_file(file_path: str) -> str:
    """Load the examples file from Google Drive."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"✅ Successfully loaded examples file ({len(content)} characters)")
        return content
    except FileNotFoundError:
        print(f"❌ Error: File not found at {file_path}")
        print("Please ensure examples.md is uploaded to the correct path in Google Drive")
        return ""
    except Exception as e:
        print(f"❌ Error loading file: {e}")
        return ""

def extract_code_blocks(content: str) -> List[Dict[str, str]]:
    """Extract JavaScript code blocks and their descriptions."""

    # Pattern to match function definitions and their code
    function_pattern = r'(// [=]{5,}\s*\n// ([A-Z\s\(\),]+)\s*\n// [=]{5,}\s*\n)(.*?)(?=\n// [=]{5,}|\nif \(typeof|$)'

    examples = []
    matches = re.finditer(function_pattern, content, re.DOTALL)

    for match in matches:
        header = match.group(1).strip()
        category = match.group(2).strip()
        code_block = match.group(3).strip()

        # Extract individual functions from the code block
        func_pattern = r'(const\s+\w+\s*=\s*async\s*\(\s*\)\s*=>\s*{.*?};)'
        func_matches = re.finditer(func_pattern, code_block, re.DOTALL)

        for func_match in func_matches:
            func_code = func_match.group(1)
            func_name = re.search(r'const\s+(\w+)', func_code).group(1)

            examples.append({
                'category': category,
                'function_name': func_name,
                'code': func_code,
                'description': f"SL2 Query API example for {category.lower()}: {func_name}"
            })

    print(f"✅ Extracted {len(examples)} code examples")
    return examples

def create_instruction_data(examples: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Create instruction-following dataset from code examples."""

    instruction_templates = [
        "Write a SL2 Query API function for {task}",
        "Create a JavaScript function using SL2 Query API to {task}",
        "Implement a SL2 database query function that {task}",
        "Show me how to {task} using SL2 Query API",
        "Generate SL2 Query API code for {task}",
        "Write a function that {task} using the SL2 JavaScript API",
        "Create a SL2 query function to {task}",
        "How do I {task} with SL2 Query API?"
    ]

    task_descriptions = {
        'BASIC FIND OPERATIONS': [
            'performs basic database queries with conditions',
            'finds records with simple and complex filters',
            'searches for records using AND/OR conditions'
        ],
        'ARRAY OPERATORS (RTL, MULTICHOICE)': [
            'queries arrays and multichoice fields',
            'searches records with array contains operations',
            'filters data using array operators'
        ],
        'RTL, REF, GLOBALREF QUERIES': [
            'queries related records using references',
            'searches using relationship fields',
            'finds records through model relationships'
        ],
        'ORDERING': [
            'sorts query results by multiple fields',
            'orders records in ascending/descending order',
            'applies custom sorting to database queries'
        ],
        'JOINS': [
            'joins multiple tables in queries',
            'performs complex multi-table operations',
            'combines data from related models'
        ],
        'GROUPING OPERATIONS': [
            'groups data and applies aggregation functions',
            'calculates statistics using GROUP BY',
            'performs data aggregation with MAX, MIN, SUM, COUNT, AVG'
        ],
        'BULK OPERATIONS': [
            'performs bulk updates on multiple records',
            'executes mass delete operations',
            'handles batch data operations efficiently'
        ],
        'PAGINATION AND LIMITS': [
            'implements pagination for large datasets',
            'limits query results with offset',
            'handles data pagination efficiently'
        ]
    }

    dataset_entries = []

    for example in examples:
        category = example['category']
        code = example['code']

        # Get task descriptions for this category
        tasks = task_descriptions.get(category, [f"works with {category.lower()}"])

        for task in tasks:
            # Random instruction template
            instruction_template = random.choice(instruction_templates)
            instruction = instruction_template.format(task=task)

            # Create training entry
            entry = {
                'instruction': instruction,
                'input': '',
                'output': code,
                'category': category,
                'function_name': example['function_name']
            }
            dataset_entries.append(entry)

    print(f"✅ Created {len(dataset_entries)} instruction-following examples")
    return dataset_entries

"""## LOAD AND PROCESS DATA"""

print("Loading examples file...")
content = load_examples_file(EXAMPLES_FILE)

if not content:
    print("❌ Cannot proceed without examples file. Please check the file path.")
    exit()

print("\nExtracting code examples...")
examples = extract_code_blocks(content)

if not examples:
    print("❌ No examples found. Please check the file format.")
    exit()

print("\nCreating instruction dataset...")
instruction_data = create_instruction_data(examples)

"""## 5. CREATE DATASETS"""

def create_chat_format(entry: Dict[str, str]) -> Dict[str, str]:
    """Convert to chat format for training."""

    messages = [
        {
            "role": "system",
            "content": "You are an expert JavaScript developer specializing in SL2 Query API. You write clean, efficient, and well-documented code following SL2 best practices."
        },
        {
            "role": "user",
            "content": entry['instruction']
        },
        {
            "role": "assistant",
            "content": entry['output']
        }
    ]

    return {
        'messages': messages,
        'category': entry['category'],
        'function_name': entry['function_name']
    }

def create_alpaca_format(entry: Dict[str, str]) -> Dict[str, str]:
    """Convert to Alpaca format for training."""
    return {
        'instruction': entry['instruction'],
        'input': entry['input'],
        'output': entry['output'],
        'category': entry['category'],
        'function_name': entry['function_name']
    }

print("Creating chat format dataset...")
chat_data = [create_chat_format(entry) for entry in instruction_data]

print("Creating Alpaca format dataset...")
alpaca_data = [create_alpaca_format(entry) for entry in instruction_data]

"""## 6. SPLIT DATASET"""

def split_dataset(data: List[Dict], train_ratio: float = 0.8, val_ratio: float = 0.1):
    """Split dataset into train/validation/test sets."""

    random.shuffle(data)
    n = len(data)

    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))

    train_data = data[:train_end]
    val_data = data[train_end:val_end]
    test_data = data[val_end:]

    return train_data, val_data, test_data

# Split both formats
train_chat, val_chat, test_chat = split_dataset(chat_data)
train_alpaca, val_alpaca, test_alpaca = split_dataset(alpaca_data)

print(f"\nDataset splits:")
print(f"Chat format - Train: {len(train_chat)}, Val: {len(val_chat)}, Test: {len(test_chat)}")
print(f"Alpaca format - Train: {len(train_alpaca)}, Val: {len(val_alpaca)}, Test: {len(test_alpaca)}")

"""## 7. CREATE HUGGING FACE DATASETS"""

# Create chat format dataset
chat_dataset = DatasetDict({
    'train': Dataset.from_list(train_chat),
    'validation': Dataset.from_list(val_chat),
    'test': Dataset.from_list(test_chat)
})

# Create Alpaca format dataset
alpaca_dataset = DatasetDict({
    'train': Dataset.from_list(train_alpaca),
    'validation': Dataset.from_list(val_alpaca),
    'test': Dataset.from_list(test_alpaca)
})

print("✅ Created Hugging Face datasets")

"""## 8. SAVE DATASETS TO GOOGLE DRIVE"""

# Save to Google Drive - Multiple locations for safety
chat_path = os.path.join(OUTPUT_DIR, f"{DATASET_NAME}-chat")
alpaca_path = os.path.join(OUTPUT_DIR, f"{DATASET_NAME}-alpaca")

print("Saving datasets to Google Drive...")
print(f"📁 Chat format will be saved to: {chat_path}")
print(f"📁 Alpaca format will be saved to: {alpaca_path}")

# Save as Hugging Face Dataset format (for training)
chat_dataset.save_to_disk(chat_path)
alpaca_dataset.save_to_disk(alpaca_path)

# Also save as JSON for inspection and backup
json_backup_dir = os.path.join(OUTPUT_DIR, "json_backups")
os.makedirs(json_backup_dir, exist_ok=True)

print("Creating JSON backups...")
with open(os.path.join(json_backup_dir, "chat_train_sample.json"), 'w') as f:
    json.dump(train_chat[:10], f, indent=2)

with open(os.path.join(json_backup_dir, "chat_full_train.json"), 'w') as f:
    json.dump(train_chat, f, indent=2)

with open(os.path.join(json_backup_dir, "alpaca_train_sample.json"), 'w') as f:
    json.dump(train_alpaca[:10], f, indent=2)

with open(os.path.join(json_backup_dir, "alpaca_full_train.json"), 'w') as f:
    json.dump(train_alpaca, f, indent=2)

# Save dataset statistics
stats = {
    "creation_date": datetime.now().isoformat(),
    "dataset_version": DATASET_VERSION,
    "original_examples": len(examples),
    "instruction_entries": len(instruction_data),
    "categories": list(set([e['category'] for e in examples])),
    "splits": {
        "train": len(train_chat),
        "validation": len(val_chat),
        "test": len(test_chat)
    },
    "category_breakdown": {cat: len([e for e in instruction_data if e['category'] == cat])
                          for cat in set([e['category'] for e in instruction_data])},
    "paths": {
        "chat_dataset": chat_path,
        "alpaca_dataset": alpaca_path,
        "json_backups": json_backup_dir
    }
}

stats_path = os.path.join(OUTPUT_DIR, "dataset_statistics.json")
with open(stats_path, 'w') as f:
    json.dump(stats, f, indent=2)

print(f"✅ Datasets saved to Google Drive:")
print(f"  📊 Chat format: {chat_path}")
print(f"  📊 Alpaca format: {alpaca_path}")
print(f"  📄 JSON backups: {json_backup_dir}")
print(f"  📈 Statistics: {stats_path}")

# Create a README file for the datasets
readme_content = f"""# SL2 Query API Training Datasets

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Version: {DATASET_VERSION}

## Dataset Information

- **Original Examples**: {len(examples)}
- **Training Entries**: {len(instruction_data)}
- **Categories**: {len(set([e['category'] for e in examples]))}

## Dataset Splits

- **Training**: {len(train_chat)} samples
- **Validation**: {len(val_chat)} samples
- **Test**: {len(test_chat)} samples

## Formats Available

### 1. Chat Format (`{DATASET_NAME}-chat/`)
For training with chat-based fine-tuning:
```python
from datasets import load_from_disk
dataset = load_from_disk('{chat_path}')
```

### 2. Alpaca Format (`{DATASET_NAME}-alpaca/`)
For training with instruction-following format:
```python
from datasets import load_from_disk
dataset = load_from_disk('{alpaca_path}')
```

### 3. JSON Backups (`json_backups/`)
Raw JSON files for inspection and backup purposes.

## Category Breakdown

{chr(10).join([f"- **{cat}**: {count} examples" for cat, count in sorted(stats['category_breakdown'].items())])}

## Usage in Training

```python
# Load for training
from datasets import load_from_disk

# For QLoRA/LoRA training (recommended)
dataset = load_from_disk('{alpaca_path}')

# For chat-based training
dataset = load_from_disk('{chat_path}')

train_dataset = dataset['train']
eval_dataset = dataset['validation']
test_dataset = dataset['test']
```

## Files Structure

```
{OUTPUT_DIR}/
├── {DATASET_NAME}-chat/           # Hugging Face Dataset (Chat format)
├── {DATASET_NAME}-alpaca/         # Hugging Face Dataset (Alpaca format)
├── json_backups/                  # JSON backup files
├── dataset_statistics.json       # Dataset statistics
└── README.md                      # This file
```

Generated by SL2 Query API Dataset Creation Pipeline
"""

readme_path = os.path.join(OUTPUT_DIR, "README.md")
with open(readme_path, 'w') as f:
    f.write(readme_content)

print(f"  📖 README: {readme_path}")

print(f"\n🎯 Google Drive Storage Summary:")
print(f"  📍 Base Path: {OUTPUT_DIR}")
print(f"  💾 Total Files: {len(os.listdir(OUTPUT_DIR))} items created")

# Calculate total size
total_size = 0
for root, dirs, files in os.walk(OUTPUT_DIR):
    for file in files:
        total_size += os.path.getsize(os.path.join(root, file))

total_size_mb = total_size / (1024 * 1024)
print(f"  📊 Total Size: {total_size_mb:.1f} MB")

"""## 9. UPLOAD TO HUGGING FACE HUB"""

def upload_to_hub():
    """Upload datasets to Hugging Face Hub."""

    try:
        print("Uploading chat format dataset to Hugging Face Hub...")
        chat_dataset.push_to_hub(
            f"{HF_REPO_NAME}-chat",
            private=False,  # Set to True if you want private dataset
            commit_description=f"SL2 Query API training dataset v{DATASET_VERSION} - Chat format"
        )

        print("Uploading Alpaca format dataset to Hugging Face Hub...")
        alpaca_dataset.push_to_hub(
            f"{HF_REPO_NAME}-alpaca",
            private=False,
            commit_description=f"SL2 Query API training dataset v{DATASET_VERSION} - Alpaca format"
        )

        print("✅ Successfully uploaded datasets to Hugging Face Hub!")
        print(f"Chat format: https://huggingface.co/datasets/{HF_REPO_NAME}-chat")
        print(f"Alpaca format: https://huggingface.co/datasets/{HF_REPO_NAME}-alpaca")

    except Exception as e:
        print(f"❌ Error uploading to Hub: {e}")
        print("You can upload manually later or check your credentials")

# Ask user if they want to upload
upload_choice = input("\nDo you want to upload datasets to Hugging Face Hub? (y/n): ").lower().strip()

if upload_choice == 'y':
    # Update HF_REPO_NAME with your actual username before uploading
    print(f"Current repo name: {HF_REPO_NAME}")
    new_repo = input("Enter your Hugging Face username/repo-name (or press Enter to use current): ").strip()
    if new_repo:
        HF_REPO_NAME = new_repo

    upload_to_hub()
else:
    print("Skipping upload. You can upload manually later.")

"""## 10. DATASET STATISTICS"""

print("\n" + "="*50)
print("DATASET CREATION SUMMARY")
print("="*50)

print(f"Original examples extracted: {len(examples)}")
print(f"Instruction entries created: {len(instruction_data)}")
print(f"Categories covered: {len(set([e['category'] for e in examples]))}")

# Category breakdown
category_counts = {}
for entry in instruction_data:
    cat = entry['category']
    category_counts[cat] = category_counts.get(cat, 0) + 1

print(f"\nCategory breakdown:")
for cat, count in sorted(category_counts.items()):
    print(f"  {cat}: {count} examples")

print(f"\nFiles created:")
print(f"  📁 {chat_path}")
print(f"  📁 {alpaca_path}")
print(f"  📄 {os.path.join(OUTPUT_DIR, 'chat_sample.json')}")
print(f"  📄 {os.path.join(OUTPUT_DIR, 'alpaca_sample.json')}")

print(f"\n✅ Dataset creation completed successfully!")
print(f"Next step: Use the training notebook with these datasets.")

"""## 11. PREVIEW SAMPLES"""

print("\n" + "="*50)
print("SAMPLE DATA PREVIEW")
print("="*50)

print("\n🔍 Chat Format Sample:")
print(json.dumps(chat_data[0], indent=2))

print("\n🔍 Alpaca Format Sample:")
print(json.dumps(alpaca_data[0], indent=2))

print("\n" + "="*50)
print("Ready for training! 🚀")
print("="*50)